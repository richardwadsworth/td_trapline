{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random, choice\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "\n",
    "    def __init__(self, env) -> None:\n",
    "        self.env = env\n",
    "\n",
    "    def action(self, q, index):\n",
    "        \"\"\"\n",
    "        implement this function in the sub class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def average_performance(self, policy_fct, q):\n",
    "    \n",
    "        acc_returns = 0.\n",
    "        n = 500\n",
    "        for i in range(n):\n",
    "            done, truncated = False, False\n",
    "            s = self.env.reset()\n",
    "            while not done and not truncated:\n",
    "                a = policy_fct(q, s)\n",
    "                s, reward, done, truncated, info = self.env.step(a)\n",
    "                acc_returns += reward\n",
    "\n",
    "        return acc_returns/n\n",
    "\n",
    "class GreedyPolicy(Policy):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def action(self, q, s):\n",
    "        return np.argmax(q[s])\n",
    "\n",
    "class SoftmaxPolicy(Policy):\n",
    "    \n",
    "    def __init__(self, env, rng):\n",
    "        super().__init__(env)\n",
    "        self.rng = rng\n",
    "\n",
    "\n",
    "    def action(self, q, s, T):\n",
    "        probs = np.exp(q[s]/T) / np.sum(np.exp(q[s]/T))\n",
    "        probs =  probs/ np.sum(probs) # Ensure probs is normalised to 1 (to avoid rounding errors)\n",
    "        randchoice = self.rng.random()\n",
    "        flag = 1; k = 1\n",
    "        while flag:\n",
    "\n",
    "            if randchoice<np.sum(probs[0:k]):\n",
    "                action = k-1 # adjust for zero based action index\n",
    "                flag = 0\n",
    "            \n",
    "            k = k + 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_action(self, T):     \n",
    "        return lambda q,s: self.action(q, s, T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for sarsa(lambda)\n",
    "episodes = 5000\n",
    "STEPS = 500\n",
    "gamma = 0.9\n",
    "\n",
    "alpha_critic = 0.02\n",
    "alpha_actor = 0.02\n",
    "\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.2\n",
    "epsilon_annealing_stop = int(episodes*0.7)\n",
    "\n",
    "eligibility_decay = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space =  Discrete(4)\n",
      "Observation space =  Discrete(256)\n",
      "Optimal policy:\n",
      "L/D/R   U     U     U  \n",
      "  L     -    L/R    -  \n",
      "  U     D     L     -  \n",
      "  -     R     D     !  \n"
     ]
    }
   ],
   "source": [
    "desc = generate_random_map(size=16, p=1.0)\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, new_step_api=True, desc=desc, max_episode_steps=STEPS)\n",
    "\n",
    "print(\"Action space = \", env.action_space)\n",
    "print(\"Observation space = \", env.observation_space)\n",
    "\n",
    "actionsDict = {}\n",
    "actionsDict[0] = \"  L  \"\n",
    "actionsDict[1] = \"  D  \"\n",
    "actionsDict[2] = \"  R  \"\n",
    "actionsDict[3] = \"  U  \"\n",
    "\n",
    "actionsDictInv = {}\n",
    "actionsDictInv[\"L\"] = 0\n",
    "actionsDictInv[\"D\"] = 1\n",
    "actionsDictInv[\"R\"] = 2\n",
    "actionsDictInv[\"U\"] = 3\n",
    "\n",
    "optimalPolicy = [\"L/D/R\",\"  U  \",\"  U  \",\"  U  \",\n",
    "                 \"  L  \",\"  -  \",\" L/R \",\"  -  \",\n",
    "                 \"  U  \",\"  D  \",\"  L  \",\"  -  \",\n",
    "                 \"  -  \",\"  R  \",\"  D  \",\"  !  \"]\n",
    "    \n",
    "print(\"Optimal policy:\")\n",
    "idxs = [0,4,8,12]\n",
    "for idx in idxs:\n",
    "    print(optimalPolicy[idx+0], optimalPolicy[idx+1], \n",
    "          optimalPolicy[idx+2], optimalPolicy[idx+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = np.ones((env.observation_space.n, env.action_space.n))\n",
    "# # Set q(terminal,*) equal to 0\n",
    "# q[5,:] = 0.0\n",
    "# q[7,:] = 0.0\n",
    "# q[11,:] = 0.0\n",
    "# q[12,:] = 0.0\n",
    "# q[15,:] = 0.0\n",
    "performance = np.zeros(episodes//STEPS) # np.ndarray(episodes//10)\n",
    "\n",
    "critic = np.random.uniform(low=0.0, high=0.0000000009, size=(env.observation_space.n, env.action_space.n))\n",
    "# critic = np.random.uniform(low=0.0, high=0.00000009, size=(env.observation_space.n, env.action_space.n))\n",
    "# v = np.random.rand(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "actor = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "# actor = np.random.uniform(low=0.0, high=0.0009, size=(env.observation_space.n, env.action_space.n))\n",
    "\n",
    "rng = np.random.default_rng() # random number generator\n",
    "policy_softmax = SoftmaxPolicy(env, rng)\n",
    "policy_greedy = GreedyPolicy(env)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "\n",
    "    inew = min(episode,epsilon_annealing_stop)\n",
    "    epsilon = (epsilon_start * (epsilon_annealing_stop - inew) + epsilon_end * inew) / epsilon_annealing_stop\n",
    "    \n",
    "    E_actor = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    E_critic = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    state = env.reset()\n",
    "    action = policy_softmax.action(actor, state, epsilon)\n",
    "    # action = action_epsilon_greedy(actor, state, epsilon)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        (new_state, reward, done, truncated, info) = env.step(action)\n",
    "        \n",
    "        new_action = policy_softmax.action(actor, new_state, epsilon)\n",
    "        # new_action = action_epsilon_greedy(actor, new_state, epsilon)\n",
    "\n",
    "        E_actor[state, action] = 1\n",
    "        \n",
    "        td_error = reward + gamma * critic[new_state, new_action] - critic[state, action]\n",
    "\n",
    "        E_critic[state, action] = 1\n",
    "\n",
    "        critic = critic + alpha_critic * td_error * E_critic\n",
    "        E_critic = eligibility_decay * gamma * E_critic\n",
    "\n",
    "        actor = actor + alpha_actor * td_error * E_actor \n",
    "        E_actor = eligibility_decay * gamma * E_actor\n",
    "        \n",
    "        state, action = new_state, new_action\n",
    "\n",
    "        if done or truncated:\n",
    "\n",
    "            # if done:\n",
    "                # print(\"done in {} steps ({})\".format(env._elapsed_steps, episode))\n",
    "\n",
    "            break\n",
    "\n",
    "    # only for plotting the performance, not part of the algorithm \n",
    "    if episode%STEPS == 0:\n",
    "        # performance[episode//100] = average_performance(get_action_epsilon_greedy(epsilon), q=actor)\n",
    "        performance[episode//STEPS] = policy_greedy.average_performance(policy_greedy.action, q=actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average reward of an epoch')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(STEPS*np.arange(episodes//STEPS), performance)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Learning progress for SARSA\")\n",
    "plt.ylabel(\"Average reward of an epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy policy SARSA performance = 1.0\n"
     ]
    }
   ],
   "source": [
    "greedyPolicyAvgPerf = policy_greedy.average_performance(policy_greedy.action, q=actor)\n",
    "print(\"Greedy policy SARSA performance =\", greedyPolicyAvgPerf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A,S) Value function = (256, 4)\n",
      "First row\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Second row\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Third row\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Fourth row\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "q = actor\n",
    "q = np.round(q,3)\n",
    "print(\"(A,S) Value function =\", q.shape)\n",
    "print(\"First row\")\n",
    "print(q[0:4,:])\n",
    "print(\"Second row\")\n",
    "print(q[4:8,:])\n",
    "print(\"Third row\")\n",
    "print(q[8:12,:])\n",
    "print(\"Fourth row\")\n",
    "print(q[12:16,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy policy found:\n",
      "  L     L     L     L  \n",
      "  L     L     L     L  \n",
      "  L     L     L     L  \n",
      "  L     L     L     !  \n",
      " \n",
      "Optimal policy:\n",
      "L/D/R   U     U     U  \n",
      "  L     -    L/R    -  \n",
      "  U     D     L     -  \n",
      "  -     R     D     !  \n"
     ]
    }
   ],
   "source": [
    "policyFound = [actionsDict[np.argmax(q[0,:])],actionsDict[np.argmax(q[1,:])],actionsDict[np.argmax(q[2,:])],actionsDict[np.argmax(q[3,:])],\n",
    "               actionsDict[np.argmax(q[4,:])],actionsDict[np.argmax(q[5,:])],actionsDict[np.argmax(q[6,:])],actionsDict[np.argmax(q[6,:])],\n",
    "               actionsDict[np.argmax(q[8,:])],actionsDict[np.argmax(q[9,:])],actionsDict[np.argmax(q[10,:])],actionsDict[np.argmax(q[11,:])],\n",
    "               actionsDict[np.argmax(q[12,:])],actionsDict[np.argmax(q[13,:])],actionsDict[np.argmax(q[14,:])],\"  !  \"]\n",
    "print(\"Greedy policy found:\")\n",
    "idxs = [0,4,8,12]\n",
    "for idx in idxs:\n",
    "    print(policyFound[idx+0], policyFound[idx+1], \n",
    "          policyFound[idx+2], policyFound[idx+3])\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Optimal policy:\")\n",
    "idxs = [0,4,8,12]\n",
    "for idx in idxs:\n",
    "    print(optimalPolicy[idx+0], optimalPolicy[idx+1], \n",
    "          optimalPolicy[idx+2], optimalPolicy[idx+3])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
